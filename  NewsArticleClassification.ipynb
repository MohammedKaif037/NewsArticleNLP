{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammedKaif037/NewsArticleNLP/blob/main/%20NewsArticleClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XaDVX7x8DXB"
      },
      "source": [
        "# News Article Classification using NLP\n",
        "\n",
        "## Introduction\n",
        "\n",
        "News articles cover a wide range of topics such as politics, economics, sports and entertainment. Automatically classifying them into categories can save significant time for journalists, readers and content aggregators.\n",
        "\n",
        "### Key Benefits of Automated News Categorization:\n",
        "- **Media Monitoring**: Quickly track news on specific topics.\n",
        "- **Content Recommendations**: Recommend articles based on users' interests.\n",
        "- **Sentiment Analysis**: Determine public sentiment towards political events, companies, etc.\n",
        "\n",
        "This can be achieved using natural language processing (NLP) by which we can classify news articles into predefined categories using text representation techniques such as **Bag of Words (BoW)** and **Term Frequency-Inverse Document Frequency (TF-IDF)**. Both techniques convert text into numerical vectors, enabling machine learning algorithms to classify news articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO2DlCY28DXG"
      },
      "source": [
        "## 1. Importing Necessary Libraries\n",
        "\n",
        "We will import the following libraries like pandas, numpy, nltk and scikit learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwKXO4b88DXH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSpUY5si8DXK"
      },
      "source": [
        "## 2. Loading the Dataset\n",
        "\n",
        "We will load the dataset into our environment and display first few rows. You can download the dataset from [here](link-to-dataset)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"amirarsalankhroush/bbc-data\")\n",
        "print(\"Dataset downloaded to:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4ZnwXty-WwX",
        "outputId": "f124543c-5d7d-4f8d-e572-bc3cf6f7ecdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'bbc-data' dataset.\n",
            "Dataset downloaded to: /kaggle/input/bbc-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxYVzSXb8DXL",
        "outputId": "b4e1362f-c1ef-47a0-d5c0-83832c8cfde2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['entertainment', 'business', 'sport', 'politics', 'tech'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df = pd.read_csv('/kaggle/input/bbc-data/bbc_data.csv')\n",
        "df[\"labels\"].unique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4vy19D38DXM"
      },
      "source": [
        "**Output:**\n",
        "```\n",
        "array(['entertainment', 'business', 'sport', 'politics', 'tech'],\n",
        "      dtype=object)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset size\n",
        "print(f\"Total articles: {len(df)}\")\n",
        "\n",
        "# Check distribution across categories\n",
        "print(\"\\nCategory distribution:\")\n",
        "print(df['labels'].value_counts())\n",
        "\n",
        "# Check a sample article\n",
        "print(\"\\nSample article (first 200 characters):\")\n",
        "print(df['data'].iloc[0][:200], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_NlooQvARzn",
        "outputId": "0e25acca-e421-4e00-f2f4-ce51ead3a9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total articles: 2225\n",
            "\n",
            "Category distribution:\n",
            "labels\n",
            "sport            511\n",
            "business         510\n",
            "politics         417\n",
            "tech             401\n",
            "entertainment    386\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample article (first 200 characters):\n",
            "Musicians to tackle US red tape  Musicians groups are to tackle US visa regulations which are blamed for hindering British acts chances of succeeding across the Atlantic.  A singer hoping to perform i ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['data'] # view data before preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "j6XN7uHRBXwI",
        "outputId": "eddfecba-d62d-4b4d-8f9d-384fc9f596ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Musicians to tackle US red tape  Musicians gro...\n",
              "1       U2s desire to be number one  U2, who have won ...\n",
              "2       Rocker Doherty in on-stage fight  Rock singer ...\n",
              "3       Snicket tops US box office chart  The film ada...\n",
              "4       Oceans Twelve raids box office  Oceans Twelve,...\n",
              "                              ...                        \n",
              "2220    Warning over Windows Word files  Writing a Mic...\n",
              "2221    Fast lifts rise into record books  Two high-sp...\n",
              "2222    Nintendo adds media playing to DS  Nintendo is...\n",
              "2223    Fast moving phone viruses appear  Security fir...\n",
              "2224    Hacker threat to Apples iTunes  Users of Apple...\n",
              "Name: data, Length: 2225, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Musicians to tackle US red tape  Musicians gro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U2s desire to be number one  U2, who have won ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Rocker Doherty in on-stage fight  Rock singer ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Snicket tops US box office chart  The film ada...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oceans Twelve raids box office  Oceans Twelve,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>Warning over Windows Word files  Writing a Mic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>Fast lifts rise into record books  Two high-sp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>Nintendo adds media playing to DS  Nintendo is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>Fast moving phone viruses appear  Security fir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>Hacker threat to Apples iTunes  Users of Apple...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2225 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-zZKhd68DXM"
      },
      "source": [
        "## 3. Downloading NLTK Resources\n",
        "\n",
        "We will download the following NLTK Resources:\n",
        "- **punkt**: A package from NLTK used for tokenizing text into words.\n",
        "- **stopwords**: A predefined list of common, meaningless words in English like \"the\", \"is\", etc.\n",
        "- **punkt_tab**: A resource for handling special tokenization cases in specific contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWvi_9Wd8DXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed0a7c8-ec52-4b4b-88b9-793345bdbdd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZaIEg2F8DXN"
      },
      "source": [
        "## 4. Preprocessing the Text\n",
        "\n",
        "We will preprocess the text data by following these steps:\n",
        "1. **Tokenization**: Split the text into individual words.\n",
        "2. **Stopword Removal**: Remove common words like \"the\", \"is\", etc. which do not add significant meaning.\n",
        "\n",
        "We will define a function for pre-processing our text.\n",
        "\n",
        "- `stopwords.words('english')`: Fetches a predefined list of common stopwords in English for filtering.\n",
        "- `word_tokenize(text.lower())`: Converts the text to lowercase and splits it into individual words (tokens).\n",
        "- `tokens = [word for word in tokens if word.isalpha()]`: Removes non-alphabetic characters (e.g., numbers, punctuation).\n",
        "- `tokens = [word for word in tokens if word not in stop_words]`: Removes stopwords to focus on meaningful words for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smbNgxeX8DXO"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lIRcX4H8DXP"
      },
      "source": [
        "## 5. Applying Preprocessing to Dataset\n",
        "\n",
        "We will now apply the preprocessing function and clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azLm5I4d8DXQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "52f07bbb-c072-48f2-aafd-71648c37ed13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [musicians, tackle, us, red, tape, musicians, ...\n",
              "1    [desire, number, one, three, prestigious, gram...\n",
              "2    [rocker, doherty, fight, rock, singer, pete, d...\n",
              "3    [snicket, tops, us, box, office, chart, film, ...\n",
              "4    [oceans, twelve, raids, box, office, oceans, t...\n",
              "Name: processed_content, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>processed_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[musicians, tackle, us, red, tape, musicians, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[desire, number, one, three, prestigious, gram...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[rocker, doherty, fight, rock, singer, pete, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[snicket, tops, us, box, office, chart, film, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[oceans, twelve, raids, box, office, oceans, t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df['processed_content'] = df['data'].apply(preprocess_text)\n",
        "df['processed_content'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5L4xlo_8DXQ"
      },
      "source": [
        "## 6. Text Vectorization\n",
        "\n",
        "Now, we will transform the text data into numerical vectors using Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
        "\n",
        "### 6.1. Vectorize Text Using BoW and TF-IDF\n",
        "\n",
        "- `CountVectorizer()`: Converts a collection of text documents into a matrix of token counts (BoW).\n",
        "- `bow_vectorizer.fit_transform()`: Fits the vectorizer on the dataset and transforms the text into a matrix of word counts.\n",
        "- `df['processed_content'].apply(' '.join)`: Joins the list of tokens (processed content) into a single string for each article, as CountVectorizer expects input in string format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5JWdLZP8DXQ"
      },
      "outputs": [],
      "source": [
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(df['processed_content'].apply(' '.join))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDwg85u08DXR"
      },
      "source": [
        "### 6.2. Vectorize Text Using TF-IDF\n",
        "\n",
        "- `TfidfVectorizer()`: Converts a collection of text documents into a matrix of TF-IDF features.\n",
        "- `tfidf_vectorizer.fit_transform()`: Fits the vectorizer on the dataset and transforms the text into a matrix of TF-IDF values.\n",
        "- `df['processed_content'].apply(' '.join)`: Joins the list of tokens (processed content) into a single string for each article, as TfidfVectorizer expects input in string format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deVEos6j8DXR"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['processed_content'].apply(' '.join))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyZuTxmg8DXS"
      },
      "source": [
        "## 7. Training our Models\n",
        "\n",
        "We will train a **Naive Bayes classifier** on the BoW and TF-IDF representations of the text. Naive Bayes is a machine learning model based on Bayes' theorem which assumes that features are independent given the class. It is particularly effective for text classification tasks where the features (words) are treated as independent predictors of the class label.\n",
        "\n",
        "- `train_test_split()`: Splits the dataset into training and test sets.\n",
        "- `test_size=0.2`: Allocates 20% of the data for testing.\n",
        "- `random_state=42`: Ensures reproducibility by using a fixed random seed.\n",
        "- `MultinomialNB()`: Initializes the Naive Bayes classifier.\n",
        "- `fit()`: Trains the model on the training data.\n",
        "\n",
        "### 7.1. Training BoW model\n",
        "\n",
        "We will train the BoW model by splitting the data into training and test sets and using a Naive Bayes classifier to fit the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxBXElxV8DXS"
      },
      "outputs": [],
      "source": [
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, df['labels'], test_size=0.2, random_state=42)\n",
        "\n",
        "nb_model1 = MultinomialNB()\n",
        "nb_model1.fit(X_train_bow, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ocbUhnc8DXS"
      },
      "source": [
        "### 7.2. Training TF-IDF model\n",
        "\n",
        "We will train the TF-IDF model by splitting the data into training and test sets and using a Naive Bayes classifier to fit the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjeEtotp8DXS"
      },
      "outputs": [],
      "source": [
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, df['labels'], test_size=0.2, random_state=42)\n",
        "\n",
        "nb_model2 = MultinomialNB()\n",
        "nb_model2.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WtGdh8u8DXT"
      },
      "source": [
        "## 8. Comparing Both Models\n",
        "\n",
        "We will now evaluate both the model's performance.\n",
        "\n",
        "- `fit()`: Trains the model on the training data.\n",
        "- `predict()`: Makes predictions on the test data.\n",
        "- `classification_report()`: Computes performance metrics like precision, recall, F1-score and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xps9Hhfc8DXT"
      },
      "outputs": [],
      "source": [
        "y_pred_bow = nb_model1.predict(X_test_bow)\n",
        "print(\"BoW Model Performance:\\n\", classification_report(y_test, y_pred_bow))\n",
        "\n",
        "y_pred_tfidf = nb_model2.predict(X_test_tfidf)\n",
        "print(\"TF-IDF Model Performance:\\n\", classification_report(y_test, y_pred_tfidf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPcowKw98DXT"
      },
      "source": [
        "### Performance Analysis\n",
        "\n",
        "The performance results from both the BoW and TF-IDF models show impressive classification accuracy across all categories.\n",
        "\n",
        "**BoW Model:**\n",
        "- Achieves an accuracy of **98%** with consistently high precision, recall and F1-scores across all categories.\n",
        "- Performs particularly well on \"sport\" and \"entertainment\" categories, showing near-perfect performance (1.00 recall and F1-score).\n",
        "\n",
        "**TF-IDF Model:**\n",
        "- Also performs well with an overall accuracy of **97%**.\n",
        "- While it performs slightly less well on the \"business\" and \"politics\" categories compared to BoW, it still demonstrates strong classification ability.\n",
        "- Shows especially high precision for the \"tech\" category.\n",
        "\n",
        "You can refer [this article](link-to-article) for more detailed difference: BoW vs TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMaEXRw78DXT"
      },
      "source": [
        "## 9. Making Predictions\n",
        "\n",
        "We will use the trained Naive Bayes models to make predictions on custom input text. The text will first be preprocessed, then transformed using both BoW and TF-IDF and finally classified into categories.\n",
        "\n",
        "- `preprocess_text(custom_text1)`: Preprocesses the input text by tokenizing, removing stopwords and filtering non-alphabetic words.\n",
        "- `' '.join(preprocess_text(custom_text1))`: Joins tokens into a string for vectorization.\n",
        "- `bow_vectorizer.transform([processed_custom_text])`: Transforms the processed text into BoW format for the model.\n",
        "- `tfidf_vectorizer.transform([processed_custom_text])`: Transforms the processed text into TF-IDF format.\n",
        "- `nb_model1.predict(custom_text_bow)`: Predicts the category using the BoW model.\n",
        "- `nb_model2.predict(custom_text_tfidf)`: Predicts the category using the TF-IDF model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp3e6PPD8DXT"
      },
      "outputs": [],
      "source": [
        "custom_text1 = \"Artificial intelligence is revolutionizing the tech industry, with companies racing to develop the next big innovation.\"\n",
        "\n",
        "print(\"Input text: \", custom_text1)\n",
        "\n",
        "processed_custom_text = ' '.join(preprocess_text(custom_text1))\n",
        "\n",
        "custom_text_bow = bow_vectorizer.transform([processed_custom_text])\n",
        "custom_text_tfidf = tfidf_vectorizer.transform([processed_custom_text])\n",
        "\n",
        "predicted_category_bow = nb_model1.predict(custom_text_bow)\n",
        "print(f\"Predicted Category (BoW): {predicted_category_bow[0]}\")\n",
        "\n",
        "predicted_category_tfidf = nb_model2.predict(custom_text_tfidf)\n",
        "print(f\"Predicted Category (TF-IDF): {predicted_category_tfidf[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OwFWU8m8DXU"
      },
      "source": [
        "**Output:**\n",
        "```\n",
        "Input text: Artificial intelligence is revolutionizing the tech industry, with companies racing to develop the next big innovation.\n",
        "Predicted Category (BoW): tech\n",
        "Predicted Category (TF-IDF): tech\n",
        "```\n",
        "\n",
        "We can see both models are working fine."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}